# Development ROADMAP

## Task 1: Cross-Modal Transformer Enhancement

### Objective
Add `cross_modal` hyperparameter to enable transformer-based fusion of visual and text tokens before classification, while maintaining backward compatibility with current averaging-based fusion.

### Implementation TODOs

#### 1.1 [âœ…] Add Cross-Modal Transformer Module
**File**: `src/model/ExMRD.py`
- Create `CrossModalTransformer` class:
  ```python
  class CrossModalTransformer(nn.Module):
      def __init__(self, d_model=256, n_heads=8, n_layers=4, d_ff=1024, dropout=0.1):
          # Transformer encoder with nn.TransformerEncoderLayer
          # cls_token: nn.Parameter(torch.zeros(1, 1, d_model))
          # modality_embeddings: nn.Embedding(2, d_model)  # 0=visual, 1=text
          # position_embeddings: nn.Parameter(torch.zeros(21, d_model))  # CLS + 16 vis + 4 text
  ```
  - Input processing:
    - Concatenate: [CLS] + visual_tokens[B,16,256] + text_tokens[B,4,256] â†’ [B,21,256]
    - Add modality embeddings: visual tokens get 0, text tokens get 1
    - Add learnable position embeddings
  - Output: Extract [CLS] token at position 0 â†’ [B,256]

#### 1.2 [âœ…] Update ExMRD Model Forward Pass
**File**: `src/model/ExMRD.py` (lines ~91-134)
- Modify token processing to preserve sequence dimension:
  ```python
  if self.cross_modal:
      # Visual: [B,16,1024] â†’ temporal_pe â†’ linear â†’ [B,16,256] (no mean pooling)
      fea_frames = self.temporal_pe(fea_frames)
      visual_tokens = self.linear_video(fea_frames)  # Keep [B,16,256]
      
      # Text: Stack 4 CoT components â†’ [B,4,256] (no mean pooling)
      text_tokens = torch.stack([fea_ocr, fea_caption, fea_comsense, fea_causal], dim=1)
      
      # Cross-modal fusion
      fused_features = self.cross_modal_transformer(visual_tokens, text_tokens)  # [B,256]
      output = self.classifier(fused_features)
  else:
      # Current approach: mean pooling then averaging (lines 130-133)
  ```

#### 1.3 [âœ…] Update Configuration System
**Files**: `src/config/ExMRD_*.yaml`
- Add under `para` section (after line 11):
  ```yaml
  para:
    hid_dim: 256
    dropout: 0.3
    text_encoder: "OFA-Sys/chinese-clip-vit-large-patch14"
    num_frozen_layers: 4
    # New cross-modal parameters
    cross_modal: false  # Toggle transformer fusion
    cross_modal_layers: 4
    cross_modal_heads: 8
    cross_modal_dim_ff: 1024
    cross_modal_dropout: 0.1
  ```

#### 1.4 [âœ…] Update Model Initialization
**File**: `src/main.py`
- Locate model initialization (search for `model = ExMRD`)
- Pass new parameters from config:
  ```python
  model = ExMRD(
      hid_dim=cfg.para.hid_dim,
      dropout=cfg.para.dropout,
      text_encoder=cfg.para.text_encoder,
      num_frozen_layers=cfg.para.num_frozen_layers,
      ablation=cfg.get('ablation', 'No'),
      ablation_no_cot=cfg.data.get('ablation_no_cot', False),
      # New parameters
      cross_modal=cfg.para.get('cross_modal', False),
      cross_modal_config={
          'n_layers': cfg.para.get('cross_modal_layers', 4),
          'n_heads': cfg.para.get('cross_modal_heads', 8),
          'd_ff': cfg.para.get('cross_modal_dim_ff', 1024),
          'dropout': cfg.para.get('cross_modal_dropout', 0.1)
      }
  )
  ```

#### 1.5 [âœ…] Test Implementation
- **Backward Compatibility Test**:
  ```bash
  python src/main.py --config-name ExMRD_FakeSV para.cross_modal=false
  # Should produce identical results to current implementation
  ```
- **New Feature Test**:
  ```bash
  python src/main.py --config-name ExMRD_FakeSV para.cross_modal=true
  # Monitor: GPU memory (+~30%), training time (+~40%), F1 score improvement
  ```
- **Ablation Study**:
  ```bash
  # Test different transformer configurations
  python src/main.py para.cross_modal=true para.cross_modal_layers=2
  python src/main.py para.cross_modal=true para.cross_modal_heads=4
  ```

### Technical Notes
- Memory complexity: O(nÂ²d) where n=21 tokens, d=256 dims
- FLOPs increase: ~21Â² Ã— 256 Ã— 4 layers = ~450K per sample
- Attention weights can be extracted for explainability analysis
- Consider gradient checkpointing if OOM occurs with batch_size=32

---
**Status**: Awaiting approval to proceed

## Task 2: Pre-trained Multimodal Model Integration

### Objective
Replace the current text encoder (Chinese-CLIP BERT) and naive cross-modal transformer with a pre-trained multimodal transformer model for end-to-end optimization. The multimodal model will encode text as token sequences and process both visual and text tokens together.

### Architecture Analysis

#### Current Flow:
```
Text (4 CoT components) â†’ Chinese-CLIP BERT â†’ Linear layers â†’ 4Ã—[B,256] â†’ Average â†’ [B,256]
Visual (16 frames) â†’ Temporal PE â†’ Linear â†’ [B,16,256] â†’ Mean â†’ [B,256]
Fusion: Simple average or CrossModalTransformer â†’ [B,256] â†’ MLP classifier
```

#### Proposed Flow:
```
Text (4 CoT components) â†’ Multimodal model tokenizer â†’ Text tokens [B,4,seq_len]
Visual (16 frames) â†’ Multimodal model vision encoder â†’ Visual tokens [B,16,768/1024]
Joint processing: Multimodal transformer â†’ Fused representation [B,d_model] â†’ MLP classifier
```

### Implementation TODOs

#### 2.1 [ ] Select and Analyze Pre-trained Multimodal Model
**Target Models** (in priority order):
1. **CLIP-based models** (OpenAI CLIP, Chinese-CLIP multimodal)
   - Good text+vision understanding
   - Existing Chinese-CLIP familiarity
   - Lightweight integration
2. **BLIP-2/InstructBLIP** (Salesforce)
   - Strong multimodal reasoning
   - Q-Former architecture for cross-modal fusion
   - Good for Chinese text with appropriate tokenizer
3. **LLaVA/Chinese-LLaVA**
   - Instruction-following multimodal model
   - Strong reasoning capabilities
   - May be overkill for classification task

**Key Requirements**:
- Support for both Chinese and English text
- Efficient batch processing
- Reasonable model size (<10B parameters)
- Available pre-trained weights

#### 2.2 [ ] Design Multimodal Model Wrapper
**File**: `src/model/MultimodalTransformer.py`
```python
class PretrainedMultimodalModel(nn.Module):
    def __init__(self, model_name, freeze_layers=None, output_dim=256):
        # Load pre-trained multimodal model
        # Configure layer freezing for efficiency
        # Add projection layer to match ExMRD dimensions
        
    def encode_text_tokens(self, text_inputs):
        # Tokenize and encode text to token sequences
        # Return: [batch_size, num_texts, seq_len, d_model]
        
    def encode_visual_tokens(self, visual_features):
        # Process visual features through vision encoder
        # Return: [batch_size, num_frames, d_model]
        
    def forward(self, text_inputs, visual_features):
        # Joint multimodal processing
        # Return: [batch_size, d_model]
```

#### 2.3 [ ] Update ExMRD Architecture
**File**: `src/model/ExMRD.py`
- Add new hyperparameter: `pretrained_multimodal`
- Conditional model selection:
  ```python
  if self.pretrained_multimodal:
      # Use pre-trained multimodal model
      self.multimodal_encoder = PretrainedMultimodalModel(...)
  elif self.cross_modal:
      # Use naive CrossModalTransformer (current Task 1)
  else:
      # Use original averaging approach
  ```
- Update forward pass with three pathways

#### 2.4 [ ] Configuration Parameters
**Files**: `src/config/ExMRD_*.yaml`
```yaml
para:
  # Existing parameters...
  
  # Pre-trained multimodal model parameters
  pretrained_multimodal: false  # Toggle pre-trained multimodal model
  multimodal_model_name: "openai/clip-vit-large-patch14"  # or "Salesforce/blip2-opt-2.7b"
  multimodal_freeze_layers: 8  # Number of layers to freeze
  multimodal_output_dim: 256   # Output projection dimension
  multimodal_max_text_len: 128  # Max text sequence length
```

#### 2.5 [ ] Data Pipeline Updates
**File**: `src/data/ExMRD_data.py`
- Update collator to handle multimodal model tokenization
- Preserve text as raw strings for multimodal tokenizer
- Ensure visual features are compatible with multimodal vision encoder

#### 2.6 [ ] Training Optimizations
**Considerations**:
- **Memory Management**: Pre-trained models are large; consider gradient checkpointing
- **Learning Rate Scheduling**: Different LR for pre-trained vs. new layers
- **Mixed Precision**: Use FP16/BF16 for efficiency
- **Layer Freezing Strategy**: Freeze early layers, fine-tune later ones

#### 2.7 [ ] Integration Testing
- **Backward Compatibility**: Ensure `pretrained_multimodal=false` works unchanged
- **Parameter Counting**: Verify significant parameter increase with multimodal model
- **Performance Comparison**: 
  - Baseline (averaging)
  - CrossModalTransformer (Task 1)
  - Pre-trained multimodal (Task 2)
- **Memory Profiling**: Monitor GPU memory usage

### Expected Benefits
1. **Stronger Representations**: Pre-trained multimodal features vs. simple averaging
2. **Better Cross-modal Understanding**: Joint optimization vs. separate encoders
3. **Transfer Learning**: Leverage large-scale pre-training for rumor detection
4. **End-to-end Optimization**: Unified model vs. pipeline approach

### Technical Challenges
1. **Model Size**: Pre-trained models may be 2-10x larger than current setup
2. **Tokenization Alignment**: Handling Chinese CoT text with different tokenizers
3. **Visual Feature Compatibility**: Current CLIP features vs. multimodal vision encoder
4. **Training Stability**: Balancing pre-trained and task-specific components

### Success Metrics
- No regression when `pretrained_multimodal=false`
- Significant parameter increase (expected: +100M to +2B parameters)
- F1 score improvement over naive cross-modal transformer
- Training remains stable and converges within reasonable time

---
**Status**: âœ… Completed

## Task 3: Enhanced Video Preprocessing Pipeline with Frame-Level Audio Processing

### Objective
Create a comprehensive preprocessing pipeline that extracts multimodal features at both frame-level and global-level for FakeSV dataset, enabling fine-grained temporal alignment between visual, audio, and text modalities.

### Implementation TODOs

#### 3.1 [âœ…] Extract and Organize Raw Video Data
**Target**: Extract videos from downloaded zip file to proper directory structure
- [âœ…] Unzip `data/FakeSV/raw_videos.zip` to `data/FakeSV/videos/`
- [âœ…] Verify video file structure matches expected format (*.mp4 files)
- [âœ…] Check video count against dataset metadata: 5,495 videos extracted (22GB)

#### 3.2 [âœ…] Video Frame Extraction with Temporal Alignment
**Target**: Extract 16 frames per video with precise temporal sampling
- [âœ…] Run frame extraction: `python preprocess/extract_frame.py`
- [âœ…] Output location: `data/FakeSV/frames_16/` (all videos completed)
- [âœ…] Record frame timestamps for audio alignment
- [âœ…] Frame format: JPG images with temporal metadata
- [âœ…] Output: Frame timestamp mapping for audio synchronization

#### 3.3 [âœ…] Frame-Aligned Audio Processing (In-Memory)
**Target**: Process audio segments corresponding to each video frame without saving intermediate files
- [âœ…] Extract full audio: `python preprocess/video_to_wav.py` (all 5,495 videos completed)
- [âœ…] Create new script: `preprocess/audio_frame_processing.py` with enhanced anomaly detection
- [âœ…] Fixed dtype compatibility issue: Whisper float16/float32 mismatch resolved
- [âœ…] Added incremental processing support: resume from existing results
- [âœ…] Output structure:
  ```
  data/FakeSV/audios/
  â””â”€â”€ {video_id}_full.wav          # Global audio only (5,495 files)
  ```
- [âœ…] Frame-level audio features saved directly to feature tensors (no intermediate .wav files)

#### 3.4 [ðŸ”„] Frame-Level and Global Audio Transcription (In-Memory)
**Target**: Generate transcripts at both frame-level and global level
- [ ] **Global transcription**: `python preprocess/wav_to_transcript.py`
  - Input: `{video_id}_full.wav`
  - Output: `data/FakeSV/transcript_global.jsonl`
- [ðŸ”„] **Frame-level transcription**: Integrate into `preprocess/audio_frame_processing.py`
  - Process: In-memory audio segments â†’ Whisper transcription
  - Output: `data/FakeSV/transcript_frames.jsonl` (currently in progress)
  - Format: `[{'vid': video_id, 'frame_transcripts': [text0, text1, ..., text15]}]`
  - Status: Currently running with fixed dtype issue

#### 3.5 [âœ…] OCR Text Extraction (Frame-Level)
**Target**: Extract on-screen text from individual video frames
- [âœ…] Create new script: `preprocess/frames_to_ocr.py` for FakeSV
- [âœ…] Process each of 16 frames individually using EasyOCR
- [âœ…] Output: `data/FakeSV/ocr_frames.jsonl` (processed available frames)
- [âœ…] Format: `[{'vid': video_id, 'frame_ocr': [ocr0, ocr1, ..., ocr15]}]`
- [âœ…] Also generate global OCR: `data/FakeSV/ocr_global.jsonl`

#### 3.6 [ ] Vision Feature Extraction (Frame-Level)
**Target**: Generate visual features for each frame using Chinese-CLIP
- [ ] Run visual feature extraction: `python preprocess/make_video_feature.py`
- [ ] Backbone: Chinese-CLIP ViT (`OFA-Sys/chinese-clip-vit-large-patch14`)
- [ ] Output: `data/FakeSV/fea/visual_features.pt`
- [ ] Feature dimensions: `[num_videos, 16, 1024]` (frame-level features)

#### 3.7 [ ] Text Feature Extraction (Frame-Level and Global)
**Target**: Generate text embeddings at multiple granularities
- [ ] **Frame-level text features**:
  - Combine: `title + frame_ocr[i] + frame_transcript[i]` for each frame
  - Use Chinese-CLIP text encoder
  - Output: `data/FakeSV/fea/text_features_frames.pt`
  - Dimensions: `[num_videos, 16, 768]`
- [ ] **Global text features**:
  - Combine: `title + global_ocr + global_transcript`
  - Output: `data/FakeSV/fea/text_features_global.pt`
  - Dimensions: `[num_videos, 768]`

#### 3.8 [ ] Audio Feature Extraction with Wav2Vec (Frame-Level and Global)
**Target**: Extract audio features using wav2vec2-base-960h at multiple levels
- [ ] Install dependencies: `transformers`, `torchaudio`
- [ ] Integrate into: `preprocess/audio_frame_processing.py`
- [ ] Model: `facebook/wav2vec2-base-960h`
- [ ] **Frame-level audio features**:
  - Process: In-memory audio segments (aligned to frame timestamps)
  - Pipeline: Full audio â†’ segment in memory â†’ wav2vec â†’ aggregate features
  - Output: `data/FakeSV/fea/audio_features_frames.pt`
  - Dimensions: `[num_videos, 16, 768]`
- [ ] **Global audio features**:
  - Process: `{video_id}_full.wav`
  - Output: `data/FakeSV/fea/audio_features_global.pt`
  - Dimensions: `[num_videos, 768]`

#### 3.9 [ ] Enhanced Data Organization Structure
**Target**: Organize all features with frame-level and global variants
```
data/FakeSV/
â”œâ”€â”€ videos/                           # Raw MP4 files
â”œâ”€â”€ frames_16/                        # Video frames with timestamps
â”‚   â”œâ”€â”€ {video_id}/
â”‚   â”‚   â”œâ”€â”€ frame_000.jpg (+ timestamp)
â”‚   â”‚   â””â”€â”€ ... (16 frames)
â”œâ”€â”€ audios/                          # Audio files
â”‚   â””â”€â”€ {video_id}_full.wav         # Global audio only
â”œâ”€â”€ fea/                            # Processed features
â”‚   â”œâ”€â”€ visual_features.pt          # [N, 16, 1024] - frame visual
â”‚   â”œâ”€â”€ text_features_frames.pt     # [N, 16, 768] - frame text  
â”‚   â”œâ”€â”€ text_features_global.pt     # [N, 768] - global text
â”‚   â”œâ”€â”€ audio_features_frames.pt    # [N, 16, 768] - frame audio
â”‚   â””â”€â”€ audio_features_global.pt    # [N, 768] - global audio
â”œâ”€â”€ ocr_frames.jsonl                # Frame-level OCR text
â”œâ”€â”€ ocr_global.jsonl                # Global OCR text  
â”œâ”€â”€ transcript_frames.jsonl         # Frame-level transcripts
â”œâ”€â”€ transcript_global.jsonl         # Global transcripts
â””â”€â”€ data.jsonl                      # Original metadata
```

#### 3.10 [ ] Audio-Frame Alignment Algorithm (In-Memory Processing)
**Target**: Precisely align audio segments with video frame timestamps for in-memory processing
- [ ] Integrate into `preprocess/audio_frame_processing.py`
- [ ] Algorithm:
  ```python
  def process_frame_aligned_audio(video_path, frame_timestamps):
      # Load full audio in memory
      # For each frame timestamp:
      #   - Calculate audio segment boundaries  
      #   - Extract audio chunk in memory (numpy array)
      #   - Process through wav2vec and whisper immediately
      #   - Store only final features/transcripts
      # Return: frame_features, frame_transcripts
  ```
- [ ] Support variable frame durations based on video length
- [ ] Process segments sequentially to minimize memory usage

#### 3.11 [ ] Raw Data Preservation
**Target**: Keep raw extracted data alongside processed features
- [ ] **Raw frame transcripts**: Store in `data/FakeSV/raw_transcripts_frames.json`
- [ ] **Raw global transcript**: Store in `data/FakeSV/raw_transcript_global.json`
- [ ] **Raw frame OCR**: Store in `data/FakeSV/raw_ocr_frames.json`
- [ ] **Raw global OCR**: Store in `data/FakeSV/raw_ocr_global.json`
- [ ] **Frame metadata**: Store timing and extraction info

#### 3.12 [ ] Multi-Granularity Data Loader
**Target**: Create flexible data loader supporting different temporal granularities
- [ ] Create `src/data/MultiGranularityDataLoader.py`
- [ ] Support modes:
  - `'frame'`: Load frame-level features `[B, 16, feat_dim]`
  - `'global'`: Load global features `[B, feat_dim]`
  - `'mixed'`: Load both frame and global features
- [ ] Modality selection: visual, text, audio, or combinations
- [ ] Temporal alignment validation across modalities

#### 3.13 [ ] Feature Quality Validation
**Target**: Comprehensive validation of extracted features
- [ ] **Temporal alignment check**: Verify audio-frame synchronization
- [ ] **Feature completeness**: Ensure all videos have all modalities
- [ ] **Dimension validation**: Check tensor shapes match expectations
- [ ] **Content quality**: Sample-check transcripts and OCR accuracy
- [ ] Generate processing report with statistics

### Technical Implementation Details

#### Frame-Level Audio Processing Script (In-Memory)
```python
# preprocess/audio_frame_processing.py
import librosa
import torch
from transformers import Wav2Vec2Processor, Wav2Vec2Model, WhisperProcessor, WhisperForConditionalGeneration

def process_frame_aligned_audio(video_path, frame_timestamps):
    """Process audio segments aligned with video frames in-memory"""
    # Load full audio
    audio, sr = librosa.load(video_path, sr=16000)
    
    frame_features = []
    frame_transcripts = []
    
    for start_time, end_time in frame_timestamps:
        # Extract segment in memory (numpy array)
        start_sample = int(start_time * sr)
        end_sample = int(end_time * sr)
        segment = audio[start_sample:end_sample]
        
        # Process through wav2vec
        features = wav2vec_model(segment)  # Extract features
        frame_features.append(features)
        
        # Generate transcript with whisper
        transcript = whisper_model(segment)  # Get transcript
        frame_transcripts.append(transcript)
    
    return torch.stack(frame_features), frame_transcripts
```

#### Memory and Storage Requirements (Optimized)
- **Visual features**: `[16K, 16, 1024]` Ã— 4 bytes â‰ˆ 1GB
- **Text features (frame)**: `[16K, 16, 768]` Ã— 4 bytes â‰ˆ 800MB
- **Text features (global)**: `[16K, 768]` Ã— 4 bytes â‰ˆ 50MB
- **Audio features (frame)**: `[16K, 16, 768]` Ã— 4 bytes â‰ˆ 800MB  
- **Audio features (global)**: `[16K, 768]` Ã— 4 bytes â‰ˆ 50MB
- **Global audio files**: 16K videos Ã— 5MB avg â‰ˆ 80GB
- **Video frames**: 16K videos Ã— 16 frames Ã— 100KB â‰ˆ 25GB
- **Text data (JSON)**: Raw transcripts, OCR â‰ˆ 1GB
- **Total storage**: ~4GB for features + ~106GB for raw data = **~110GB total**

#### Processing Pipeline Order (Optimized)
1. Extract raw videos and frames (with timestamps)
2. Extract global audio files
3. **In-memory frame-level processing**: For each video:
   - Load audio â†’ segment by timestamps â†’ wav2vec features + whisper transcripts â†’ save results
4. Extract OCR text (frame-level and global)  
5. Generate visual features (frame-level)
6. Generate text features (frame-level and global)
7. Process global audio features
8. Validate and organize all outputs

### Success Criteria
- [ ] Frame-level temporal alignment achieved across all modalities
- [ ] Both frame-level `[N, 16, feat_dim]` and global `[N, feat_dim]` features extracted
- [ ] Audio segments properly aligned with video frame timestamps
- [ ] Raw data preserved alongside processed features
- [ ] Multi-granularity data loader supports flexible feature access
- [ ] Complete processing pipeline documented and reproducible

---
**Status**: Awaiting approval to proceed

## Task 4: Entity-Claim Knowledge Base Extraction for FakeSV Videos

### Objective
Extract entities and their associated factual claims from FakeSV videos using GPT-4o-mini. Build a streamlined knowledge base for RAG-based fact-checking, where each entity has contextual descriptions and point-by-point claims with source attribution.

### Architecture Overview

#### Input Components per Video:
1. **Visual**: 4 composite frames (2x2 grid layout, covering 16 frames total)
2. **Text**: Title, keywords, overall transcript
3. **Metadata**: Video label (çœŸ/å‡/è¾Ÿè°£)

#### Output Components:
1. **Per Video**:
   - Entities mentioned/shown
   - Factual claims made (point by point)
   - Video description
   - Temporal evolution

2. **Aggregated Knowledge Base**:
   - Entity â†’ Description with context
   - Entity â†’ List of all factual claims
   - Claim â†’ Source video IDs

### Implementation (Completed)

#### 4.1 [âœ…] Phase 1: Extract from True Videos (annotation='çœŸ')
**File**: `preprocess/extract_entity_claims.py`
- Implemented complete extraction pipeline with GPT-4o-mini
- Processes videos with checkpoint support and progress tracking
- Generates entity-claim mappings with 5W principle (Who/What/When/Where/Why)
- Includes entity merging and normalization
- Creates LLM-based entity descriptions from aggregated claims
- Outputs: `video_extractions.jsonl`, `entity_knowledge_base.json`, `claim_index.json`

#### 4.2 [ðŸ”„] Phase 2: Extract from Fake Videos (annotation='å‡')
**File**: `preprocess/extract_entity_claims.py`
**Implementation Plan**:

##### Step 1: Initial Description Extraction
- Input: Fake news videos with title, keywords, frames
- Process with GPT-4o-mini to extract ONLY:
  - Video description
  - Temporal evolution
- NO entity/claim extraction in this step

##### Step 2: Find Most Similar True News
- Concatenate: `title + keywords + description + temporal_evolution`
- Encode using Chinese-CLIP (`OFA-Sys/chinese-clip-vit-large-patch14`)
- Build vector bank from all true news (same concatenation + encoding)
- Calculate pairwise cosine similarity
- Extract top-k most similar true news videos

##### Step 3: Extract False Claims with Context
- Input: Fake video data + most similar true news
- Provide LLM with:
  - Current fake video information
  - All entities from similar true news
  - All correct claims for each entity
- Ask LLM to extract false claims for existing entities
- Allow new entity false claims (mark as "LLM-generated")
- Update `entity_knowledge_base.json` with false claims

##### Step 4: Summarize False Claims
- Merge entities with same normalization
- Generate comprehensive false claim summaries per entity
- Update final `entity_knowledge_base.json`

#### 4.3 [ ] Design Prompt Template
**Key Elements**:
```python
prompt = f"""
åˆ†æžè¿™ä¸ªçŸ­è§†é¢‘çš„å†…å®¹ï¼Œæå–çŸ¥è¯†å›¾è°±ä¿¡æ¯ã€‚

è§†é¢‘ä¿¡æ¯ï¼š
- æ ‡é¢˜ï¼š{title}
- å…³é”®è¯ï¼š{keywords}  
- éŸ³é¢‘è½¬å½•ï¼š{transcript}
- æ ‡ç­¾ï¼š{label}
- å‘å¸ƒæ—¶é—´ï¼š{publish_time} # æ ¼å¼åŒ–ä¸ºYYYY-MM-DD

è§†é¢‘å¸§æ—¶åºä¿¡æ¯ï¼š
è¿™ä¸ªè§†é¢‘åŒ…å«4å¼ å¤åˆå¸§å›¾åƒï¼Œæ¯å¼ åŒ…å«4ä¸ªè¿žç»­å¸§ï¼ˆ2x2ç½‘æ ¼å¸ƒå±€ï¼‰ï¼š
- ç¬¬1å¼ å¤åˆå¸§ï¼ˆå¸§0-3ï¼Œè§†é¢‘å¼€å§‹éƒ¨åˆ†ï¼‰ï¼š
  å·¦ä¸Š(1,1): å¸§0  |  å³ä¸Š(1,2): å¸§1
  å·¦ä¸‹(2,1): å¸§2  |  å³ä¸‹(2,2): å¸§3
- ç¬¬2å¼ å¤åˆå¸§ï¼ˆå¸§4-7ï¼Œè§†é¢‘å‰ä¸­éƒ¨åˆ†ï¼‰ï¼š
  å·¦ä¸Š(1,1): å¸§4  |  å³ä¸Š(1,2): å¸§5
  å·¦ä¸‹(2,1): å¸§6  |  å³ä¸‹(2,2): å¸§7
- ç¬¬3å¼ å¤åˆå¸§ï¼ˆå¸§8-11ï¼Œè§†é¢‘åŽä¸­éƒ¨åˆ†ï¼‰ï¼š
  å·¦ä¸Š(1,1): å¸§8  |  å³ä¸Š(1,2): å¸§9
  å·¦ä¸‹(2,1): å¸§10 |  å³ä¸‹(2,2): å¸§11
- ç¬¬4å¼ å¤åˆå¸§ï¼ˆå¸§12-15ï¼Œè§†é¢‘ç»“æŸéƒ¨åˆ†ï¼‰ï¼š
  å·¦ä¸Š(1,1): å¸§12 |  å³ä¸Š(1,2): å¸§13
  å·¦ä¸‹(2,1): å¸§14 |  å³ä¸‹(2,2): å¸§15

æ³¨æ„ï¼šå¦‚æžœéŸ³é¢‘è½¬å½•çœ‹èµ·æ¥æ²¡æœ‰æ„ä¹‰ï¼Œå¯èƒ½æ˜¯å› ä¸ºéŸ³é¢‘åªæ˜¯èƒŒæ™¯éŸ³ä¹è€Œæ²¡æœ‰äººå£°ã€‚

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼ˆç”¨ä¸­æ–‡å›žç­”ï¼‰ï¼š

1. å®žä½“ï¼ˆEntitiesï¼‰ï¼š
   åˆ—å‡ºè§†é¢‘ä¸­çš„ä¸»è¦å®žä½“ï¼ˆäººç‰©ã€åœ°ç‚¹ã€ç»„ç»‡ã€äº‹ä»¶ã€æ¦‚å¿µç­‰ï¼‰

2. äº‹å®žæ€§å£°ç§°ï¼ˆFactual Claimsï¼‰ï¼š
   åˆ—å‡ºè§†é¢‘ä¸­æ‰€æœ‰çš„äº‹å®žæ€§å£°ç§°ï¼Œè¦å…·ä½“ã€å¯éªŒè¯
   æ ¼å¼ï¼šæ¯ä¸ªå£°ç§°ç‹¬ç«‹æˆè¡Œï¼Œç®€æ´æ˜Žäº†
   ä¾‹å¦‚ï¼š
   - "æ­¦æ±‰å‘çŽ°æ–°åž‹å† çŠ¶ç—…æ¯’"
   - "ç–«è‹—æœ‰æ•ˆçŽ‡è¾¾åˆ°90%"
   - "æŸå›½ç¡®è¯Šç—…ä¾‹è¶…è¿‡100ä¸‡"

3. è§†é¢‘æè¿°ï¼ˆDescriptionï¼‰ï¼š
   ç®€è¦æè¿°è¿™ä¸ªè§†é¢‘è®²è¿°äº†ä»€ä¹ˆï¼ˆ50-100å­—ï¼‰

4. æ—¶åºæ¼”å˜ï¼ˆTemporal Evolutionï¼‰ï¼š
   æè¿°å†…å®¹å¦‚ä½•åœ¨4ä¸ªå¤åˆå¸§ä¸­æ¼”å˜

è¿”å›žJSONæ ¼å¼ï¼š
{
  "entities": ["å®žä½“1", "å®žä½“2", ...],
  "claims": [
    "å…·ä½“çš„äº‹å®žæ€§å£°ç§°1",
    "å…·ä½“çš„äº‹å®žæ€§å£°ç§°2",
    ...
  ],
  "description": "è§†é¢‘æè¿°",
  "temporal_evolution": "æ—¶åºæ¼”å˜æè¿°"
}
"""
```

#### 4.4 [ ] Implement Data Processing Pipeline
**Components**:
1. **Data Loading**:
   ```python
   def load_fakesv_data(filter_label='çœŸ'):
       # Load from data_complete_orig.jsonl
       # Filter by annotation
       # Load corresponding transcripts and OCR
   ```

2. **Image Processing**:
   ```python
   def prepare_composite_frames(video_id):
       # Load 4 quad images from data/FakeSV/quads_4/
       # Convert to base64 for GPT-4o-mini vision
   ```

3. **API Integration**:
   ```python
   def extract_knowledge_graph(video_data, images):
       # Call GPT-4o-mini with vision capability
       # Parse JSON response
       # Handle errors and retries
   ```

#### 4.5 [ ] Storage Format Design
**Output Structure**:
```
data/FakeSV/entity_claims/
â”œâ”€â”€ video_extractions.jsonl   # Per-video extraction results
â”œâ”€â”€ entity_knowledge_base.json # Aggregated entity descriptions and claims
â””â”€â”€ claim_index.json          # Claim to video mapping
```

**Data Structure Definitions**:

1. **video_extractions.jsonl** (æ¯è¡Œä¸€ä¸ªè§†é¢‘çš„æå–ç»“æžœ):
```json
{
  "video_id": "3x4zj7hyemptkvm",
  "annotation": "çœŸ",
  "title": "è§†é¢‘æ ‡é¢˜",
  "entities": ["ä¸­å›½", "æ–°å† ç–«è‹—", "æ­¦æ±‰"],
  "claims": [
    "ä¸­å›½æˆåŠŸç ”å‘æ–°å† ç–«è‹—",
    "ç–«è‹—æœ‰æ•ˆçŽ‡è¾¾åˆ°95%",
    "æ­¦æ±‰æ˜¯ç–«æƒ…é¦–å‘åœ°"
  ],
  "description": "è¯¥è§†é¢‘å±•ç¤ºäº†ä¸­å›½æ–°å† ç–«è‹—ç ”å‘çš„è¿‡ç¨‹å’Œæˆæžœã€‚",
  "temporal_evolution": "ç¬¬1å¸§å±•ç¤ºå®žéªŒå®¤ï¼Œç¬¬2å¸§å±•ç¤ºä¸´åºŠè¯•éªŒï¼Œç¬¬3å¸§å±•ç¤ºæ•°æ®åˆ†æžï¼Œç¬¬4å¸§å±•ç¤ºç–«è‹—æŽ¥ç§ã€‚"
}
```

2. **entity_knowledge_base.json** (å®žä½“çŸ¥è¯†åº“):
```json
{
  "ä¸­å›½": {
    "correct_description": "åŸºäºŽçœŸå®žè§†é¢‘ä¸­çš„æ­£ç¡®å£°ç§°ï¼šä¸­å›½åœ¨ç–«æƒ…æœŸé—´å®žæ–½äº†ä¸¥æ ¼çš„é˜²æŽ§æŽªæ–½ï¼ŒæˆåŠŸç ”å‘äº†å¤šæ¬¾æ–°å† ç–«è‹—ï¼Œç–«è‹—æŽ¥ç§çŽ‡ä½å±…ä¸–ç•Œå‰åˆ—ã€‚",
    "false_description": "åŸºäºŽè™šå‡è§†é¢‘ä¸­çš„é”™è¯¯å£°ç§°ï¼š[å¾…åŽç»­ä»Žå‡æ–°é—»è§†é¢‘ä¸­æå–]",
    "correct_claims": [
      {
        "claim": "ä¸­å›½æˆåŠŸç ”å‘æ–°å† ç–«è‹—",
        "video_ids": ["video1", "video3", "video7"],
        "source_label": "çœŸ"
      },
      {
        "claim": "ä¸­å›½ç–«æƒ…é˜²æŽ§æŽªæ–½åŒ…æ‹¬å°åŸŽå’Œå…¨æ°‘æ ¸é…¸",
        "video_ids": ["video2", "video5"],
        "source_label": "çœŸ"
      }
    ],
    "false_claims": [
      {
        "claim": "[å¾…ä»Žå‡æ–°é—»è§†é¢‘ä¸­æå–]",
        "video_ids": [],
        "source_label": "å‡"
      }
    ],
    "video_count": {
      "çœŸ": 156,
      "å‡": 0,  // å¾…åŽç»­å¤„ç†
      "è¾Ÿè°£": 0  // å¾…åŽç»­å¤„ç†
    },
    "first_seen": "2020-01-15",
    "last_seen": "2023-12-20"
  },
  "æ–°å† ç–«è‹—": {
    "correct_description": "åŸºäºŽçœŸå®žè§†é¢‘ï¼šæ–°å† ç–«è‹—ç»è¿‡ä¸‰æœŸä¸´åºŠè¯•éªŒï¼Œæœ‰æ•ˆçŽ‡åœ¨60-95%ä¹‹é—´ï¼Œå¯æ˜¾è‘—é™ä½Žé‡ç—‡çŽ‡å’Œæ­»äº¡çŽ‡ã€‚",
    "false_description": "åŸºäºŽè™šå‡è§†é¢‘ï¼š[å¾…åŽç»­ä»Žå‡æ–°é—»è§†é¢‘ä¸­æå–]",
    "correct_claims": [
      {
        "claim": "ç–«è‹—æœ‰æ•ˆçŽ‡è¾¾åˆ°95%",
        "video_ids": ["video1", "video4"],
        "source_label": "çœŸ"
      },
      {
        "claim": "ç–«è‹—å¯é¢„é˜²é‡ç—‡",
        "video_ids": ["video6", "video8", "video9"],
        "source_label": "çœŸ"
      }
    ],
    "false_claims": [],
    "video_count": {
      "çœŸ": 89,
      "å‡": 0,
      "è¾Ÿè°£": 0
    },
    "first_seen": "2020-03-10",
    "last_seen": "2023-11-15"
  }
}
```

3. **claim_index.json** (å£°ç§°ç´¢å¼•):
```json
{
  "correct_claims": [
    {
      "id": "C001",
      "claim": "ä¸­å›½æˆåŠŸç ”å‘æ–°å† ç–«è‹—",
      "entities": ["ä¸­å›½", "æ–°å† ç–«è‹—"],
      "video_ids": ["video1", "video3", "video7"],
      "frequency": 3,
      "source_label": "çœŸ"
    },
    {
      "id": "C002",
      "claim": "ç–«è‹—æœ‰æ•ˆçŽ‡è¾¾åˆ°95%",
      "entities": ["æ–°å† ç–«è‹—"],
      "video_ids": ["video1", "video4"],
      "frequency": 2,
      "source_label": "çœŸ"
    }
  ],
  "false_claims": [
    // å¾…åŽç»­ä»Žå‡æ–°é—»è§†é¢‘ä¸­æå–
  ],
  "statistics": {
    "total_correct_claims": 523,
    "total_false_claims": 0,  // å¾…åŽç»­å¤„ç†
    "total_videos_processed": {
      "çœŸ": 1814,
      "å‡": 0,
      "è¾Ÿè°£": 0
    }
  }
}
```


#### 4.6 [ ] Quality Control & Validation
- **Entity Normalization**: Merge similar entities (e.g., "ä¸­å›½" and "ä¸­åŽäººæ°‘å…±å’Œå›½")
- **Claim Deduplication**: Merge similar claims across videos
- **Claim Validation**: Ensure claims are factual and verifiable
- **Coverage Metrics**: Track extraction success rate and completeness

#### 4.7 [ ] Description Generation from Claims
- **Correct Description**: Aggregate and summarize all correct claims for each entity
- **False Description**: [To be implemented when processing fake videos]
- **Contextual Enhancement**: Add temporal and source context to descriptions
- **Description Validation**: Ensure descriptions are coherent and informative

### Configuration Parameters
```yaml
entity_claim_extraction:
  api_key: "${OPENAI_API_KEY}"
  model: "gpt-4o-mini"
  temperature: 0.3
  max_tokens: 800
  batch_size: 10
  retry_attempts: 3
  filter_labels: ["çœŸ"]  # Start with real videos
  vision_detail: "high"
  rate_limit_delay: 1.0  # seconds between API calls
```

### Testing Plan

#### 4.8 [ ] Initial Testing (10 samples)
1. Select 10 diverse videos with annotation='çœŸ'
2. Run extraction pipeline
3. Manually validate results
4. Calculate metrics:
   - Entity extraction precision
   - Relation extraction accuracy
   - Description relevance
   - Temporal coherence

#### 4.9 [ ] Scale Testing (100 samples)
1. Process 100 videos after initial validation
2. Analyze common entity types and relations
3. Build entity/relation vocabulary
4. Identify edge cases and errors

#### 4.10 [ ] Full Dataset Processing
1. Process all 1,814 'çœŸ' labeled videos
2. Optional: Process 'å‡' and 'è¾Ÿè°£' videos
3. Generate statistics and insights
4. Create visualization of knowledge graph

### Expected Outputs

1. **Per Video**:
   - 5-10 entities on average
   - 3-8 factual claims
   - 1 general description
   - 1 temporal evolution description

2. **Dataset Level**:
   - Entity vocabulary (~500-1000 unique entities)
   - Unique claims (~2000-3000)
   - Entity knowledge base with contextual descriptions
   - Claim-to-video mapping for source attribution

### Technical Considerations

1. **API Costs**: 
   - ~$0.15 per 1M input tokens (GPT-4o-mini)
   - ~$0.60 per 1M output tokens
   - Estimated: ~$50 for full dataset

2. **Processing Time**:
   - ~2-3 seconds per video (API latency)
   - ~1.5 hours for full dataset (with rate limiting)

3. **Error Handling**:
   - Retry logic for API failures
   - Fallback to text-only if vision fails
   - Logging for debugging

### Success Criteria
- [ ] Successfully extract entities and claims for 95%+ videos
- [ ] Average entity count > 5 per video
- [ ] Average claim count > 3 per video
- [ ] Clear separation between correct and false claims
- [ ] Entity descriptions accurately summarize claims
- [ ] Database structure supports efficient retrieval

---
**Status**: Awaiting approval to proceed