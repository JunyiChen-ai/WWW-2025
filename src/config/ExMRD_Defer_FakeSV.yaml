train:

data:
  tokenizer_name: "OFA-Sys/chinese-clip-vit-large-patch14"
  include_piyao: false  # Whether to include '辟谣' (rumor debunking) labels, mapped to real (0)
  description: false      # If true, include description in text concatenation
  temp_evolution: false   # If true, include temporal_evolution in text concatenation
  # Modality control parameters
  use_text: true         # If true, use text features
  use_image: true        # If true, use visual/image features
  use_audio: true        # If true, use audio features
  # Defer-specific parameters
  llm_predictions_path: "data/FakeSV/entity_claims/gating_predictions/gating_predictions.json"
  label_smoothing_epsilon: 0.01  # Label smoothing for LLM predictions
  filter_k: 20        # Number of most similar training samples to filter per test sample (null for no filtering)
  llm_independent: true  # If true, use LLM independent predictions (no-slm mode)
  
para:
  hid_dim: 256
  dropout: 0.3
  text_encoder: "OFA-Sys/chinese-clip-vit-large-patch14"
  num_frozen_layers: 4
  # Base model architecture parameters (ExMRD_Evidential)
  use_evidential: true  # If false, use traditional concatenation + classifier
  # Evidential model specific parameters
  evidential_hidden: 256
  evidential_dropout: 0.1
  anneal_steps: 1000
  loss_weights:
    fused: 1.0
    text: 1
    audio: 0.5
    image: 0.5
  # Defer-specific parameters
  gate_hidden_dim: 256    # Hidden dimension for GateNet MLP
  gate_dropout: 0.1       # Dropout for GateNet
  defer_threshold: 0.5    # Threshold τ for deferral decision (π > τ → defer to LLM)
  label_smoothing_epsilon: 0.01  # ε for label smoothing LLM predictions

opt:
  name: AdamW
  lr: 5e-5              # Learning rate for joint SLM + Gate training  
  weight_decay: 5e-4
  
sche:
  name: DummyLR
  
num_epoch: 15           # Number of training epochs
batch_size: 32          # Batch size
text_encoder: "OFA-Sys/chinese-clip-vit-large-patch14"
seed: 2024
model: ExMRD_Defer      # Use defer model
dataset: FakeSV
type: temporal          # Use temporal split for evaluation
patience: 5             # Early stopping patience

# Evaluation parameters
eval_only: false        # Set to true for evaluation only mode
checkpoint: log/FakeSV_k20_0824-210320  # Path to checkpoint for evaluation only mode

# Additional notes:
# - This config trains the learn-to-defer model end-to-end
# - The defer threshold can be tuned on validation set if needed
# - LLM predictions are loaded from the gating_predictions.json file
# - Both SLM and Gate are trained jointly with AdamW optimizer
# - Loss = L_defer + L_evidential (combined training objective)